# -*- coding: utf-8 -*-
"""OpenAi-Chat Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sfPKn_vcSE3i4UzjmyL9AI3v7baUruSz
"""

# Step 1.1: Upgrade pip and install essential libraries

# pip is the package manager for Python â€” it lets us install new tools or libraries
# The first command upgrades pip to the latest version to avoid compatibility issues
!pip install --upgrade pip

# We now install three essential libraries:
# - transformers: This library provides access to powerful AI models that understand and generate human-like text.
# - accelerate: Helps make these models run faster and more efficiently, especially on machines with GPUs (graphics processors).
# - gradio: A tool that lets us build a simple, interactive web app so users can chat with the AI using a browser.
!pip install transformers accelerate gradio

# Step 1.2: Check if a GPU is available

# GPUs (Graphics Processing Units) are specialized chips that speed up AI computations.
# Using a GPU instead of a regular processor (CPU) makes the chatbot respond much faster.
# We use PyTorch (a popular AI framework) to check if a GPU is available on this computer.
import torch
print("GPU available:", torch.cuda.is_available())  # This will print "True" if a GPU is present, "False" if not.

# Step 2: Load and configure the chatbot AI model

# First, we import the necessary components from the Hugging Face Transformers library:
# - AutoTokenizer: This tool breaks down user input into tokens (small chunks) so the model can understand it.
# - AutoModelForCausalLM: Loads a language model that predicts the next word in a conversation (Causal Language Modeling).
# - pipeline: A simple wrapper to bundle the model and tokenizer into an easy-to-use chatbot.
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# We specify which model to use from Hugging Faceâ€™s online repository.
# 'Qwen/Qwen3-4B' is a small but powerful chatbot model developed by Alibaba that works well on local machines.
MODEL_NAME = "Qwen/Qwen3-4B"

# Step 2.1: Authenticate with Hugging Face if the model requires permission (some models are private).
# huggingface_hub provides access to your account so it knows you're allowed to download private models.
from huggingface_hub import login

# Replace this token with your own from https://huggingface.co/settings/tokens
# It authenticates your access to Hugging Face services.
login(token="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")

# Step 2.2: Load the tokenizer and model from Hugging Face

# The tokenizer converts regular text (like "Hello, how are you?") into a format the model understands.
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)

# The model is the core of the chatbot â€” it's been trained to generate human-like responses.
# We load it using options that help optimize for performance:
# - device_map="auto": Automatically uses GPU if available, otherwise uses CPU.
# - torch_dtype="auto": Automatically picks the best data type (e.g., float32 or float16) depending on hardware support.
# - trust_remote_code=True: Allows downloading model-specific code from Hugging Face if needed.
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=True
)

# Step 2.3: Create a chatbot pipeline (wrapper to generate replies)

# The pipeline combines both the model and tokenizer, making it easy to use with just one function call.
# It is configured to generate a reply with:
# - max_new_tokens=512: The chatbot wonâ€™t generate responses longer than 512 words/tokens.
# - temperature=0.7: Controls creativity (lower = more accurate, higher = more random).
# - top_p=0.9: Filters out unlikely words to make responses more natural and focused.
llama_pipe = pipeline(
    "text-generation",  # Task type
    model=model,        # Pre-trained AI model
    tokenizer=tokenizer,  # Text tokenizer
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9
)

# Step 1: Install dependencies

# These libraries help the chatbot search the internet and extract readable summaries from websites.
# - googlesearch-python: Allows us to perform Google searches in Python.
# - newspaper3k: Automatically downloads and extracts readable content (like article text) from news/blog pages.
# - lxml_html_clean: Helps clean and parse messy HTML content if needed.
!pip install --upgrade googlesearch-python newspaper3k lxml_html_clean


# Step 2: Import required modules

# The search function from googlesearch lets us find web pages related to a user query (like Google does).
from googlesearch import search

# The Article class from newspaper helps us download and clean up full article text from a web page.
from newspaper import Article

# The logging module helps us record errors without crashing the program (useful if a web page fails to load).
import logging


# Step 3: Define a function that performs a web search and returns summarized content

def web_search(query, max_results=5, snippet_chars=300):
    """
    This function enables the chatbot to provide real-world, up-to-date information by:
    1. Searching Google for the user's question or topic.
    2. Visiting the top results and extracting readable article content.
    3. Returning a short snippet from each article so the chatbot can summarize it for the user.

    Parameters:
    - query (str): The search term or question the user types.
    - max_results (int): Limits how many search results we process (default is 5).
    - snippet_chars (int): Length of the preview/snippet from each article (default is 300 characters).

    Returns:
    - A list of dictionaries. Each dictionary contains:
        - title: The articleâ€™s title
        - href: The URL (web address)
        - body: A short preview of the article text
    """

    results = []  # This will store all the extracted article summaries

    # Perform the search using Google and process each result
    for url in search(query, num_results=max_results, lang="en"):
        try:
            # Try to download and extract the content of the article
            art = Article(url)
            art.download()
            art.parse()
            text = art.text.replace("\n", " ").strip()  # Clean the text by removing line breaks

            # Create a short preview/snippet of the article
            snippet = text[:snippet_chars] + ("â€¦" if len(text) > snippet_chars else "")

            # Store the information (title, link, preview)
            results.append({
                "title": art.title or url,  # If no title is found, use the URL instead
                "href":  url,
                "body":  snippet
            })

        except Exception as e:
            # If anything goes wrong (site doesnâ€™t load, bad format, etc.), log the error and skip
            logging.warning(f"Failed to parse {url}: {e}")

        # Stop early if weâ€™ve already collected enough articles
        if len(results) >= max_results:
            break

    return results  # Return the list of article summaries

# Step 4: Define a function to generate an AI answer using live web search results with proper citations

def answer_with_citations(user_query):
    """
    This function enables the chatbot to respond to a user's question using information gathered
    from the internet in real time. It also ensures that the answer includes source links (citations),
    so users know where the information came from.

    Input:
    - user_query (str): The actual question the user types in.

    Output:
    - answer (str): A well-written response from the chatbot that explains the answer using live data
      and includes source citations.
    """

    # Step 4.1: Perform a Google search to find the top 10 web pages related to the userâ€™s question.
    # The results contain each pageâ€™s title, a brief text summary, and its URL.
    hits = web_search(user_query, max_results=10)

    # Step 4.2: Prepare the prompt (instruction) that will be sent to the AI chatbot model.

    # Start building a list of strings. This will become one large paragraph.
    # The first line tells the AI how to behave: it should use the results to form a clear answer
    # and provide source links.
    prompt_parts = [
        "You are an AI assistant. Use the following search results to answer the question below. "
        "Explain clearly, list important points from the search results, and provide citations in the format [number].\n\n"
    ]

    # Step 4.3: Add each search result (title, URL, and snippet) to the prompt.
    # We format each result with a number and include the title, a snippet of the text, and the URL.
    for i, hit in enumerate(hits):
        prompt_parts.append(f"[{i+1}] \"{hit['title']}\": {hit['body']} ({hit['href']})\n")

    # Step 4.4: Add the user's original question to the prompt.
    # This tells the AI what it needs to answer based on the provided information.
    prompt_parts.append(f"\nQuestion: {user_query}")

    # Step 4.5: Combine all the parts of the prompt into a single string.
    # This complete string will be sent to the AI model.
    prompt = "".join(prompt_parts)

    # Step 4.6: Get the AI's answer using the prepared prompt.
    # We use the previously defined llama_pipe (from Step 2) to get the response from the model.
    # The result is a list of generated texts; we take the first one and extract the 'generated_text'.
    result = llama_pipe(prompt)[0]['generated_text']

    # Step 4.7: Extract only the AIâ€™s answer from the full generated text.
    # The model often includes the prompt itself in the output. We need to find where the actual answer starts.
    # We look for the "Question: [user_query]" part and take everything that comes after it.
    # If the "Question:" part isn't found (which is unlikely), we just use the whole generated text.
    answer_start_index = result.find(f"\nQuestion: {user_query}")
    if answer_start_index != -1:
      answer = result[answer_start_index + len(f"\nQuestion: {user_query}"):].strip()
    else:
      answer = result.strip()


    # Step 4.8: Add citations back to the AI's answer.
    # The AI model might remove the citation numbers. We need to re-add them based on the original search results.
    # This is a simplified way to add citations back. A more advanced method would match sentences
    # in the answer to the search result snippets they came from.

    # Create a dictionary mapping URL to citation number for easy lookup.
    url_to_citation = {hit['href']: f"[{i+1}]" for i, hit in enumerate(hits)}

    # Add citations at the end of the answer, listing each source URL.
    # This ensures users can click on the links to verify the information.
    citations_text = "\n\nSources:\n"
    for i, hit in enumerate(hits):
        citations_text += f"[{i+1}] {hit['href']}\n"

    # Combine the AI's answer and the list of citations.
    final_answer = answer + citations_text

    return final_answer

# Step 1: Import Gradio â€” a tool to create interactive web apps in Python
import gradio as gr

# Step 2: Define the main function that powers the chatbot logic
def chat_fn(user_message, history):
    """
    Handles a new message from the user and updates the conversation history.

    Parameters:
    - user_message: The latest question the user typed.
    - history: A list of all previous messages (as pairs of user input and bot reply).

    Returns:
    - Updated history with the new question and AI-generated answer.
    """

    # Add the user's new message to the chat history (reply is temporarily set to None)
    history = history + [(user_message, None)]

    # Call the answer_with_citations function to get the AIâ€™s response based on web search
    bot_reply = answer_with_citations(user_message)

    # Replace the placeholder None with the actual bot reply
    history[-1] = (user_message, bot_reply)

    # Return the updated chat history (twice â€” once for display, once for internal state)
    return history, history


# Step 3: Define custom CSS styles to make the chat bubbles look modern and clean
css = """
/* Align the entire interface to the center of the page */
.gradio-container { display: flex; justify-content: center; padding: 2rem; }

/* Limit the max width of the chat box for better readability */
.chat-box { width: 100%; max-width: 700px; }

/* Styling for the user message bubbles (blue with white text) */
.chatbot .message.user {
  background-color: #007AFF;
  color: white;
  border-radius: 16px 16px 0 16px;
  padding: 8px 12px;
  margin: 4px 0;
  align-self: flex-end;
  max-width: 80%;
}

/* Styling for the AI bot reply bubbles (orange with dark text) */
.chatbot .message.bot {
  background-color: #FFA500;
  color: #111;
  border-radius: 16px 16px 16px 0;
  padding: 8px 12px;
  margin: 4px 0;
  align-self: flex-start;
  max-width: 80%;
}

/* Style for the text input box */
input[type="text"] {
  border-radius: 20px;
  padding: 10px 16px;
  border: 1px solid #CCC;
}
"""


# Step 4: Create the Gradio interface using Blocks layout
with gr.Blocks(css=css, title="ðŸ“¡ LLaMAâ€¯3.2â€¯3B Webâ€‘Search Agent") as demo:

    # Show a title or instruction at the top
    gr.Markdown("## Ask anything and get cited answers")

    # Grouping the chat UI elements together and applying styling
    with gr.Group(elem_classes="chat-box"):

        # The visual chat window (displays the conversation)
        chatbot = gr.Chatbot(elem_classes="chatbot", height=500)

        # A hidden state that stores the ongoing chat as a list of messages
        state = gr.State([])

        # Textbox where the user types their message
        user_input = gr.Textbox(
            placeholder="Type your question and hit enter",
            show_label=False  # We hide the label for a cleaner look
        )

    # When the user hits Enter after typing a question:
    # â†’ Call chat_fn(user_input, state) â†’ return new chat + state â†’ update display
    user_input.submit(
        fn=chat_fn,
        inputs=[user_input, state],
        outputs=[chatbot, state]
    )

# Step 5: Launch the chatbot app in the browser
# `share=True` will generate a public link you can use to share with others
demo.launch(share=True)

